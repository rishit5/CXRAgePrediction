{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rishitratan5/age-prediction-of-chest-x-ray-images-using-resnet?scriptVersionId=161500904\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"7d77ae21","metadata":{"papermill":{"duration":0.004984,"end_time":"2024-02-03T06:06:04.738432","exception":false,"start_time":"2024-02-03T06:06:04.733448","status":"completed"},"tags":[]},"source":["# Age Prediction of Patients by Chest X-rays"]},{"cell_type":"code","execution_count":1,"id":"9e46f6e1","metadata":{"execution":{"iopub.execute_input":"2024-02-03T06:06:04.74934Z","iopub.status.busy":"2024-02-03T06:06:04.748936Z","iopub.status.idle":"2024-02-03T06:06:10.086958Z","shell.execute_reply":"2024-02-03T06:06:10.085975Z"},"papermill":{"duration":5.346099,"end_time":"2024-02-03T06:06:10.089241","exception":false,"start_time":"2024-02-03T06:06:04.743142","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import torch\n","import torchvision\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","from torchvision.utils import save_image\n","from torchvision.transforms import ToTensor, Compose, Resize, Grayscale, Normalize, Lambda\n","import os\n","import torchvision.transforms as T\n","import pydicom\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from torch.utils.data import DataLoader\n","from torch import optim\n","from sklearn.metrics import roc_auc_score\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"id":"d9ab4c70","metadata":{"execution":{"iopub.execute_input":"2024-02-03T06:06:10.099208Z","iopub.status.busy":"2024-02-03T06:06:10.098778Z","iopub.status.idle":"2024-02-03T06:06:10.103016Z","shell.execute_reply":"2024-02-03T06:06:10.102191Z"},"papermill":{"duration":0.011285,"end_time":"2024-02-03T06:06:10.104891","exception":false,"start_time":"2024-02-03T06:06:10.093606","status":"completed"},"tags":[]},"outputs":[],"source":["device = torch.device('cuda')"]},{"cell_type":"code","execution_count":3,"id":"d7dc3e14","metadata":{"execution":{"iopub.execute_input":"2024-02-03T06:06:10.113827Z","iopub.status.busy":"2024-02-03T06:06:10.113503Z","iopub.status.idle":"2024-02-03T06:06:10.117871Z","shell.execute_reply":"2024-02-03T06:06:10.117043Z"},"papermill":{"duration":0.010931,"end_time":"2024-02-03T06:06:10.1198","exception":false,"start_time":"2024-02-03T06:06:10.108869","status":"completed"},"tags":[]},"outputs":[],"source":["# Define the training image directory and annotations csv\n","train_img_dir = '/kaggle/input/minijsrtageprediction/XPAge01_RGB/XP/JPGs/'\n","train_annotations_file = '/kaggle/input/minijsrtageprediction/XPAge01_RGB/XP/trainingdata.csv'\n","\n","# Define the testing image directory and annotations csv\n","test_img_dir = '/kaggle/input/minijsrtageprediction/XPAge01_RGB/XP/JPGs/'\n","test_annotations_file = '/kaggle/input/minijsrtageprediction/XPAge01_RGB/XP/testdata.csv'\n","\n","# Define the output classes here\n","# num_classes = 100"]},{"cell_type":"code","execution_count":4,"id":"36eaa619","metadata":{"execution":{"iopub.execute_input":"2024-02-03T06:06:10.128405Z","iopub.status.busy":"2024-02-03T06:06:10.128136Z","iopub.status.idle":"2024-02-03T06:06:10.156404Z","shell.execute_reply":"2024-02-03T06:06:10.155601Z"},"papermill":{"duration":0.034639,"end_time":"2024-02-03T06:06:10.158311","exception":false,"start_time":"2024-02-03T06:06:10.123672","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["89"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(test_annotations_file)\n","df['age'].max()"]},{"cell_type":"code","execution_count":5,"id":"5823f714","metadata":{"execution":{"iopub.execute_input":"2024-02-03T06:06:10.16739Z","iopub.status.busy":"2024-02-03T06:06:10.16713Z","iopub.status.idle":"2024-02-03T06:06:10.174297Z","shell.execute_reply":"2024-02-03T06:06:10.173463Z"},"papermill":{"duration":0.013964,"end_time":"2024-02-03T06:06:10.176214","exception":false,"start_time":"2024-02-03T06:06:10.16225","status":"completed"},"tags":[]},"outputs":[],"source":["# Define a custom class for dataset \n","class CustomDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_dir + self.img_labels.iloc[idx, 0]\n","        image = read_image(img_path)\n","        image = T.ToPILImage() (image)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label"]},{"cell_type":"code","execution_count":6,"id":"9f3f7084","metadata":{"execution":{"iopub.execute_input":"2024-02-03T06:06:10.185181Z","iopub.status.busy":"2024-02-03T06:06:10.18489Z","iopub.status.idle":"2024-02-03T06:06:10.190704Z","shell.execute_reply":"2024-02-03T06:06:10.189876Z"},"papermill":{"duration":0.012392,"end_time":"2024-02-03T06:06:10.192517","exception":false,"start_time":"2024-02-03T06:06:10.180125","status":"completed"},"tags":[]},"outputs":[],"source":["# Definining the resnet18 model\n","from torchvision.models import resnet18, ResNet18_Weights\n","import torch.nn as nn\n","class Resnet18Classifier(nn.Module):\n","    def __init__(self):\n","        super(Resnet18Classifier, self).__init__()\n","        self.resnet18 = resnet18(pretrained=True).eval()\n","        num_ftrs = self.resnet18.fc.out_features\n","        self.fc = nn.Linear(num_ftrs, 1)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.fc(self.resnet18(x))"]},{"cell_type":"code","execution_count":7,"id":"c10504a0","metadata":{"execution":{"iopub.execute_input":"2024-02-03T06:06:10.201415Z","iopub.status.busy":"2024-02-03T06:06:10.201148Z","iopub.status.idle":"2024-02-03T06:06:10.213146Z","shell.execute_reply":"2024-02-03T06:06:10.212494Z"},"papermill":{"duration":0.018406,"end_time":"2024-02-03T06:06:10.214911","exception":false,"start_time":"2024-02-03T06:06:10.196505","status":"completed"},"tags":[]},"outputs":[],"source":["# Defining the transform for loading the data\n","transform = Compose([\n","    ToTensor(),\n","    Lambda(lambda x: x.repeat(3,1,1)),\n","    Resize(256)\n","])\n","\n","# Defining the training dataset and data loader\n","train_dataset = CustomDataset(\n","    annotations_file=train_annotations_file,\n","    img_dir=train_img_dir,\n","    transform=transform,\n",")\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# Defining the testing dataset and data loader\n","test_dataset = CustomDataset(\n","    annotations_file=test_annotations_file,\n","    img_dir=test_img_dir,\n","    transform=transform\n",")\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":8,"id":"b8cf5d23","metadata":{"execution":{"iopub.execute_input":"2024-02-03T06:06:10.223886Z","iopub.status.busy":"2024-02-03T06:06:10.223614Z","iopub.status.idle":"2024-02-03T06:06:11.752208Z","shell.execute_reply":"2024-02-03T06:06:11.751293Z"},"papermill":{"duration":1.535431,"end_time":"2024-02-03T06:06:11.754394","exception":false,"start_time":"2024-02-03T06:06:10.218963","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 48.8MB/s]\n"]},{"data":{"text/plain":["Resnet18Classifier(\n","  (resnet18): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=512, out_features=1000, bias=True)\n","  )\n","  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Initializing the resnet18 classifier with number of output models\n","resnet_18_classifier = Resnet18Classifier().to(device)\n","\n","# Defining the loss_function and the optimizer\n","loss_func = nn.L1Loss()\n","optimizer = optim.AdamW(resnet_18_classifier.parameters(), lr = 0.01)\n","resnet_18_classifier"]},{"cell_type":"code","execution_count":9,"id":"b571112b","metadata":{"execution":{"iopub.execute_input":"2024-02-03T06:06:11.766602Z","iopub.status.busy":"2024-02-03T06:06:11.766267Z","iopub.status.idle":"2024-02-03T06:10:47.509896Z","shell.execute_reply":"2024-02-03T06:10:47.508699Z"},"papermill":{"duration":275.75247,"end_time":"2024-02-03T06:10:47.512419","exception":false,"start_time":"2024-02-03T06:06:11.759949","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/100], Loss: 79.6564\n","Epoch [2/100], Loss: 33.7119\n","Epoch [3/100], Loss: 28.8661\n","Epoch [4/100], Loss: 13.6859\n","Epoch [5/100], Loss: 28.4021\n","Epoch [6/100], Loss: 17.0776\n","Epoch [7/100], Loss: 58.8713\n","Epoch [8/100], Loss: 119.8967\n","Epoch [9/100], Loss: 12.5335\n","Epoch [10/100], Loss: 19.4335\n","Epoch [11/100], Loss: 12.8801\n","Epoch [12/100], Loss: 13.0153\n","Epoch [13/100], Loss: 13.0299\n","Epoch [14/100], Loss: 14.2978\n","Epoch [15/100], Loss: 11.6814\n","Epoch [16/100], Loss: 16.8158\n","Epoch [17/100], Loss: 12.4375\n","Epoch [18/100], Loss: 16.5389\n","Epoch [19/100], Loss: 17.8750\n","Epoch [20/100], Loss: 12.3015\n","Epoch [21/100], Loss: 18.4701\n","Epoch [22/100], Loss: 13.4871\n","Epoch [23/100], Loss: 13.0680\n","Epoch [24/100], Loss: 13.0657\n","Epoch [25/100], Loss: 13.4167\n","Epoch [26/100], Loss: 13.1368\n","Epoch [27/100], Loss: 15.6793\n","Epoch [28/100], Loss: 10.1878\n","Epoch [29/100], Loss: 10.4531\n","Epoch [30/100], Loss: 8.1250\n","Epoch [31/100], Loss: 11.8750\n","Epoch [32/100], Loss: 12.4375\n","Epoch [33/100], Loss: 12.2409\n","Epoch [34/100], Loss: 13.6875\n","Epoch [35/100], Loss: 12.5680\n","Epoch [36/100], Loss: 14.3514\n","Epoch [37/100], Loss: 13.7892\n","Epoch [38/100], Loss: 13.5591\n","Epoch [39/100], Loss: 9.1170\n","Epoch [40/100], Loss: 16.8871\n","Epoch [41/100], Loss: 11.7421\n","Epoch [42/100], Loss: 9.0945\n","Epoch [43/100], Loss: 14.0848\n","Epoch [44/100], Loss: 13.8454\n","Epoch [45/100], Loss: 13.2500\n","Epoch [46/100], Loss: 13.9593\n","Epoch [47/100], Loss: 16.8694\n","Epoch [48/100], Loss: 14.6887\n","Epoch [49/100], Loss: 14.1995\n","Epoch [50/100], Loss: 12.2174\n","Epoch [51/100], Loss: 12.1920\n","Epoch [52/100], Loss: 11.7457\n","Epoch [53/100], Loss: 12.7743\n","Epoch [54/100], Loss: 12.0126\n","Epoch [55/100], Loss: 10.9361\n","Epoch [56/100], Loss: 10.5724\n","Epoch [57/100], Loss: 12.9847\n","Epoch [58/100], Loss: 12.6222\n","Epoch [59/100], Loss: 15.2705\n","Epoch [60/100], Loss: 13.2192\n","Epoch [61/100], Loss: 12.6354\n","Epoch [62/100], Loss: 10.0316\n","Epoch [63/100], Loss: 15.5375\n","Epoch [64/100], Loss: 11.3943\n","Epoch [65/100], Loss: 15.6274\n","Epoch [66/100], Loss: 10.3174\n","Epoch [67/100], Loss: 11.9386\n","Epoch [68/100], Loss: 12.5562\n","Epoch [69/100], Loss: 16.9713\n","Epoch [70/100], Loss: 19.8756\n","Epoch [71/100], Loss: 10.9029\n","Epoch [72/100], Loss: 11.0850\n","Epoch [73/100], Loss: 9.4049\n","Epoch [74/100], Loss: 10.0468\n","Epoch [75/100], Loss: 7.4127\n","Epoch [76/100], Loss: 13.5724\n","Epoch [77/100], Loss: 10.1250\n","Epoch [78/100], Loss: 11.0001\n","Epoch [79/100], Loss: 13.3125\n","Epoch [80/100], Loss: 12.0824\n","Epoch [81/100], Loss: 11.1017\n","Epoch [82/100], Loss: 16.7429\n","Epoch [83/100], Loss: 12.6788\n","Epoch [84/100], Loss: 12.1062\n","Epoch [85/100], Loss: 12.5582\n","Epoch [86/100], Loss: 12.6366\n","Epoch [87/100], Loss: 14.2819\n","Epoch [88/100], Loss: 12.1618\n","Epoch [89/100], Loss: 11.8605\n","Epoch [90/100], Loss: 11.3518\n","Epoch [91/100], Loss: 13.8069\n","Epoch [92/100], Loss: 14.7756\n","Epoch [93/100], Loss: 11.1015\n","Epoch [94/100], Loss: 14.5420\n","Epoch [95/100], Loss: 13.4710\n","Epoch [96/100], Loss: 15.8125\n","Epoch [97/100], Loss: 16.3125\n","Epoch [98/100], Loss: 12.7746\n","Epoch [99/100], Loss: 14.1341\n","Epoch [100/100], Loss: 11.4784\n","Training finished\n"]}],"source":["# Training the model\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = resnet_18_classifier(images)\n","        loss = loss_func(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","print(\"Training finished\")"]},{"cell_type":"code","execution_count":10,"id":"b78c1790","metadata":{"execution":{"iopub.execute_input":"2024-02-03T06:10:47.543377Z","iopub.status.busy":"2024-02-03T06:10:47.542972Z","iopub.status.idle":"2024-02-03T06:10:53.632005Z","shell.execute_reply":"2024-02-03T06:10:53.630879Z"},"papermill":{"duration":6.106904,"end_time":"2024-02-03T06:10:53.634063","exception":false,"start_time":"2024-02-03T06:10:47.527159","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy for age: 11.86\n"]}],"source":["# Testing the model\n","test_loss = 0.0\n","with torch.no_grad():\n","    resnet_18_classifier.eval()\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = resnet_18_classifier(images)\n","        loss = loss_func(outputs, labels)\n","        test_loss += loss.item()\n","        \n","\n","accuracy = test_loss / len(test_loader)\n","print(f\"Test Accuracy for age: {accuracy:.2f}\")"]},{"cell_type":"code","execution_count":null,"id":"fe94bab7","metadata":{"papermill":{"duration":0.012048,"end_time":"2024-02-03T06:10:53.659211","exception":false,"start_time":"2024-02-03T06:10:53.647163","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3927016,"sourceId":6829743,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":294.643775,"end_time":"2024-02-03T06:10:55.959056","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-03T06:06:01.315281","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}